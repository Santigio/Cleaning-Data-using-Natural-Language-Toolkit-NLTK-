# -*- coding: utf-8 -*-
"""Cleaning Text Data Using NLTK.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1XKC2Py8tMKpnZQp05jbO7jTsi_h8cWDw

# Text Cleaning in Python
"""

#warnings :)
import warnings
warnings.filterwarnings('ignore')

#Creating bunch of sentences
raw_docs = ["Hello Ben! How are you? These are just basic texts.",
"I'm just writing it for the demo PURPOSE.",
"The point is to _learn demonstrate how to use NLTK to perform cleansing_ on #simple # data."]

#importing nltk package
import nltk

nltk.download()

"""# Step 1 - convert to lower case"""

import string
raw_docs = [doc.lower() for doc in raw_docs]
print(raw_docs)

"""# Step 2 - Tokenization"""

# word tokenize
from nltk.tokenize import word_tokenize
tokenized_docs = [word_tokenize(doc) for doc in raw_docs]
print(tokenized_docs)

#Sentence tokenization

from nltk.tokenize import sent_tokenize
sent_token = [sent_tokenize(doc) for doc in raw_docs]
print(sent_token)

"""# Step 3 - Punctuation Removal"""

# Removing punctuation
import re
regex = re.compile('[%s]' % re.escape(string.punctuation)) #see documentation here: http://docs.python.org/2/library/string.html

tokenized_docs_no_punctuation = []

for review in tokenized_docs:
    new_review = []
    for token in review:
        new_token = regex.sub(u'', token)
        if not new_token == u'':
            new_review.append(new_token)
    
    tokenized_docs_no_punctuation.append(new_review)
    
print(tokenized_docs_no_punctuation)

"""# Step 4 - Removing Stopwords"""

# Cleaning text of stopwords
from nltk.corpus import stopwords

tokenized_docs_no_stopwords = []

for doc in tokenized_docs_no_punctuation:
    new_term_vector = []
    for word in doc:
        if not word in stopwords.words('english'):
            new_term_vector.append(word)
    
    tokenized_docs_no_stopwords.append(new_term_vector)

print(tokenized_docs_no_stopwords)

"""# Step 5- Stemming and Lemmantization"""

# Stemming and Lemmatization
from nltk.stem.porter import PorterStemmer
from nltk.stem.wordnet import WordNetLemmatizer

porter = PorterStemmer()
wordnet = WordNetLemmatizer()

preprocessed_docs = []

for doc in tokenized_docs_no_stopwords:
    final_doc = []
    for word in doc:
        final_doc.append(porter.stem(word))
        #final_doc.append(wordnet.lemmatize(word))
    
    preprocessed_docs.append(final_doc)

print(preprocessed_docs)

"""# Advance cleaning technique 1 - Normalization """

text = "F.Y.I, On the 30th Jan 2020 30 January 2020, W.H.O declared the Ebola a Public Health Emergency and allocated fund of 490,000,000 U.S.D"

!pip install normalise
from normalise import normalise

custom_abbr = {
    "W.H.O": "World Health Organization",
    "D.L.S": "United States Dollars",
    "F.Y.I":"For your information"
    
}

normalized_tokens = normalise(word_tokenize(text), user_abbrevs=custom_abbr, verbose=False)
display(f"Normalized text: {' '.join(normalized_tokens)}")